#!/usr/bin/env python
# coding: utf-8

# # ç’°å¢ƒè¨­ç½®

# ## è¼‰å…¥å¥—ä»¶
# - seleniumã€pandasã€requestsã€bs4ã€time

# In[1]:


from selenium import webdriver
from selenium.webdriver.chrome.options import Options  # è¨­å®š driver çš„è¡Œç‚º
from selenium.webdriver.support.ui import Select  # é¸æ“‡ï¼‚ä¸‹æ‹‰å¼é¸å–®ï¼‚
from selenium.webdriver.common.keys import Keys  # éµç›¤æ“ä½œ
from selenium.common.exceptions import NoSuchElementException, TimeoutException  # è¼‰å…¥å¸¸è¦‹éŒ¯èª¤
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities  # æ›´æ”¹è¼‰å…¥ç­–ç•¥
from selenium.webdriver.support.ui import WebDriverWait  # ç­‰å¾…æ©Ÿåˆ¶
from selenium.webdriver.support import expected_conditions as EC  # é æœŸäº‹ä»¶
from selenium.webdriver.common.by import By  # æ‰¾å°‹å…ƒç´ çš„æ–¹æ³•
import pandas as pd  # è¼‰å…¥ pandas
import pandas.io.formats.excel  # è¼¸å‡ºè‡ªå®šç¾©æ ¼å¼ Excel
import requests
from bs4 import BeautifulSoup
import time  # å¼·åˆ¶ç­‰å¾…


# ## è¨­å®š driver çš„åƒæ•¸ï¼šoptionsã€desired_capabilities

# In[2]:


if __name__ == '__main__':
    my_options = Options()
    my_options.add_argument('--incognito')  # é–‹å•Ÿç„¡ç—•æ¨¡å¼
    # my_options.add_argument('--start-maximized')  # è¦–çª—æœ€å¤§åŒ–
    # my_options.add_argument('--headless')  # ä¸é–‹å•Ÿå¯¦é«”ç€è¦½å™¨
    my_capabilities = DesiredCapabilities.CHROME
    my_capabilities['pageLoadStrategy'] = 'eager'  # é é¢åŠ è¼‰ç­–ç•¥ï¼šHTML è§£ææˆ DOM


# # è‡ªå®šç¾©å‡½å¼

# ## organize_columns(df1)
# - è™•ç†æ¬„ä½ï¼šåœ–æ›¸é¤¨ã€é¤¨è—åœ°ï¼ˆc2ï¼‰ã€ç´¢æ›¸è™Ÿï¼ˆc3ï¼‰ã€é¤¨è—ç‹€æ…‹ï¼ˆc4ï¼‰ã€é€£çµ
# - å¦‚æœ df1 æ˜¯è£è‘— DataFrame çš„ listï¼Œå‰‡å°±åˆä½µå®ƒå€‘ï¼›å¦å‰‡ï¼ˆdf1 æ˜¯ DataFrameï¼‰ï¼Œå°±æ¥è‘—åŸ·è¡Œä»¥ä¸‹æ•˜è¿°ã€‚
# - ä¸Ÿæ‰åƒåœ¾æ¬„ä½ï¼Œæ•´ç†æˆè¦å‘ˆç¾çš„è¡¨æ ¼
# - æ–°å¢å¿…è¦æ¬„ä½ï¼ˆåœ–æ›¸é¤¨ã€é€£çµï¼‰
# - å¡«æ»¿ NaNï¼ˆç”¨ ffill çš„ æ–¹å¼ï¼‰

# In[3]:


def organize_columns(df1):
    # åˆä½µå…¨éƒ¨çš„ DataFrame
    try:
        df1 = pd.concat(df1, axis=0, ignore_index=True)
    except:
        df1.reset_index(drop=True, inplace=True)

    # è™•ç† column 2ï¼šé¤¨è—åœ°
    c2 = [
        'åˆ†é¤¨/å°ˆå®¤', 'é¤¨è—åœ°/å®¤', 'é¤¨è—å®¤', 'é¤¨è—åœ°/é¤¨è—å®¤', 'é¤¨è—åœ°', 'å…¸è—é¤¨', 'é¤¨è—ä½ç½®', 'é¤¨è—åœ°/å€åŸŸ',
        'å…¸è—åœ°åç¨±', 'é¤¨è—åœ°/é¤¨åˆ¥', 'é¤¨è—åœ°(å·²å¤–å€Ÿ/ç¸½æ•¸)', 'é¤¨è—åœ°/å€åŸŸLocation', 'ç¾è¡Œä½ç½®'
    ]
    df1['c2'] = ''
    for c in c2:
        try:
            df1['c2'] += df1[c]
        except:
            pass

    # è™•ç† column 3ï¼šç´¢æ›¸è™Ÿ
    c3 = ['ç´¢æ›¸è™Ÿ', 'ç´¢æ›¸è™Ÿ/æœŸåˆŠåˆè¨‚æœ¬å·æœŸ', 'ç´¢æ›¸è™Ÿ / éƒ¨å†Šè™Ÿ', 'ç´¢æ›¸è™ŸCall No.']
    df1['c3'] = ''
    for c in c3:
        try:
            df1['c3'] += df1[c]
        except:
            pass

    # è™•ç† column 4ï¼šé¤¨è—ç‹€æ…‹
    c4 = [
        'é¤¨è—ä½ç½®(åˆ°æœŸæ—¥æœŸåƒ…ç‚ºæœŸé™ï¼Œä¸ä»£è¡¨ä¸Šæ¶æ—¥æœŸ)', 'ç‹€æ…‹/åˆ°æœŸæ—¥', 'ç›®å‰ç‹€æ…‹ / åˆ°æœŸæ—¥', 'é¤¨è—ç‹€æ…‹', 'è™•ç†ç‹€æ…‹',
        'ç‹€æ…‹ (èªªæ˜)', 'é¤¨è—ç¾æ³ èªªæ˜', 'ç›®å‰ç‹€æ…‹/é è¨ˆæ­¸é‚„æ—¥æœŸ', 'åœ–æ›¸ç‹€æ³ / åˆ°æœŸæ—¥', 'èª¿é–±èªªæ˜', 'å€Ÿé–±ç‹€æ…‹',
        'ç‹€æ…‹', 'é¤¨è—ç‹€æ…‹(æœˆ-æ—¥-è¥¿å…ƒå¹´)', 'åœ–æ›¸ç‹€æ³', 'ç¾æ³/ç•°å‹•æ—¥', 'Unnamed: 24', 'åœ–æ›¸ç‹€æ³Book Status', 'é¤¨è—ç‹€æ³(æœˆ-æ—¥-è¥¿å…ƒå¹´)'
    ]
    df1['c4'] = ''
    for c in c4:
        try:
            df1['c4'] += df1[c]
        except:
            pass

    # ç›´æ¥ç”Ÿæˆå¦ä¸€å€‹ DataFrame
    df2 = pd.DataFrame()
    df2['åœ–æ›¸é¤¨'] = df1['åœ–æ›¸é¤¨']
    df2['é¤¨è—åœ°'] = df1['c2']
    df2['ç´¢æ›¸è™Ÿ'] = df1['c3']
    df2['é¤¨è—ç‹€æ…‹'] = df1['c4']
    df2['é€£çµ'] = df1['é€£çµ']

    # é‡åˆ°å€¼ç‚º NaNæ™‚ï¼Œå°‡å‰ä¸€åˆ—çš„å€¼å¡«è£œé€²ä¾†
    df2.fillna(method="ffill", axis=0, inplace=True)

    return df2


# ## set_excel(df, directory) æš«åœ
# - å¾…

# In[4]:


def set_excel(df, directory):
    # Bï½¢åœ–æ›¸é¤¨ï½£ã€Cã€Œé¤¨è—åœ°ã€ã€Dã€Œç´¢æ›¸è™Ÿã€ã€Eã€Œé¤¨è—ç‹€æ…‹ã€ã€Fã€Œé€£çµã€
    pandas.io.formats.excel.header_style = None  # æ¨™é¡Œæ ¼å¼æ¸…é™¤
    writer = pd.ExcelWriter(directory)
    df.to_excel(writer, sheet_name="æœå°‹çµæœ")

    workbook1 = writer.book
    worksheets = writer.sheets
    worksheet1 = worksheets["æœå°‹çµæœ"]

    # æ¸¬è©¦
    cell_format = workbook1.add_format({
        "font_name": "å¾®è»Ÿæ­£é»‘é«”",
        "font_size": 16,
        "align": "left",
        #         "border": 80,
    })
    worksheet1.set_column("B:F", 40, cell_format)

    # è¨­å®šå–®å…ƒæ ¼çš„å¯¬åº¦
    #     worksheet1.set_column("B:F", 35)

    writer.save()
    print("çˆ¬å–å®Œæˆ")


# ## wait_for_element_present(driver, element_position, waiting_time=5, by=By.CSS_SELECTOR)
# - ç”¨æ³•ï¼š
#     - ç­‰å¾… element å‡ºç¾ï¼Œæ¯é–“éš” 0.5 ç§’å®šä½ä¸€æ¬¡ï¼Œç›´åˆ° 5 ç§’ã€‚å¦‚æœå®šä½ element æˆåŠŸï¼Œå›å‚³ elementï¼›å¦å‰‡ï¼Œå›å‚³ Noneã€‚
# - åƒæ•¸ï¼š
#     - driver
#     - element_positionï¼šå…ƒç´ ä½ç½®ï¼Œé è¨­ CSS selector
#     - waiting_timeï¼šç­‰å¾…æ™‚é–“ï¼Œé è¨­ 5 ç§’
#     - byï¼šå®šä½æ–¹å¼ï¼Œé è¨­ By.CSS_SELECTOR

# In[5]:


def wait_for_element_present(driver, element_position, waiting_time=5, by=By.CSS_SELECTOR):
    try:
        element = WebDriverWait(driver, waiting_time).until(
            EC.presence_of_element_located((by, element_position)))
    except:
        return
    else:
        return element


# ## wait_for_element_clickable(driver, element_position, waiting_time=5, by=By.LINK_TEXT)
# - åŒä¸Š

# In[6]:


def wait_for_element_clickable(driver, element_position, waiting_time=5, by=By.LINK_TEXT):
    try:
        time.sleep(0.3)
        element = WebDriverWait(driver, waiting_time).until(
            EC.element_to_be_clickable((by, element_position)))
    except:
        return
    else:
        return element


# ## wait_for_url_changed(driver, old_url, waiting_time=10)
# - ç”¨æ³•ï¼š
#     - ç­‰å¾…ç¶²å€æ”¹è®Šï¼ˆè¼¸å…¥çš„ç¶²å€å’Œç¾åœ¨çš„ç¶²å€é€²è¡Œæ¯”è¼ƒï¼‰ï¼Œæ¯é–“éš” 0.5 ç§’æª¢æŸ¥ä¸€æ¬¡ï¼Œç›´åˆ° 10 ç§’ã€‚å¦‚æœç¶²å€æœ‰æ”¹è®Šï¼Œå›å‚³ Trueï¼›å¦å‰‡ï¼Œå›å‚³ Noneã€‚
# - åƒæ•¸ï¼š
#     - driver
#     - old_urlï¼šèˆŠç¶²å€
#     - waiting_timeï¼šç­‰å¾…æ™‚é–“ï¼Œé è¨­ 10 ç§’

# In[7]:


def wait_for_url_changed(driver, old_url, waiting_time=10):
    try:
        WebDriverWait(driver, waiting_time).until(EC.url_changes(old_url))
    except:
        return
    else:
        return True


# ## accurately_find_table_and_read_it(driver, table_position, table_index=0)
# - ç”¨æ³•ï¼š
#     - ç²¾æº–å®šä½ table ä¸¦è®€å–æˆ pd.DataFrameã€‚å¦‚æœå®šä½ table æˆåŠŸï¼Œå›å‚³ tableï¼›å¦å‰‡ï¼Œå›å‚³ Noneã€‚
#     - ç‚º table å¢åŠ ï¼‚åœ–æ›¸é¤¨ï¼‚å’Œï¼‚é€£çµï¼‚çš„æ¬„ä½ã€‚
# - åƒæ•¸ï¼š
#     - table_positionï¼štable ä½ç½®ï¼Œé è¨­ CSS selector

# In[8]:


def accurately_find_table_and_read_it(driver, table_position, table_index=0):
    try:
        if not wait_for_element_present(driver, table_position):
            print(f'æ‰¾ä¸åˆ° {table_position}ï¼')
            return
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        table_innerHTML = soup.select(table_position)[table_index]
        tgt = pd.read_html(str(table_innerHTML), encoding='utf-8')[0]
        # tgt['åœ–æ›¸é¤¨'], tgt['é€£çµ'] = org, driver.current_url
    except:
        return
    else:
        return tgt


# ## select_ISBN_strategy(driver, select_position, option_position, waiting_time=30, by=By.NAME)
# - ç”¨æ³•ï¼š
#     - ç­‰å¾… select å‡ºç¾ï¼Œä¸¦é¸æ“‡ä»¥ ISBN æ–¹å¼æœå°‹
# - åƒæ•¸ï¼š
#     - driver
#     - select_positionï¼šselect ä½ç½®ï¼Œé è¨­ name
#     - option_positionï¼šoption ä½ç½®ï¼Œé è¨­ value
#     - waiting_timeï¼šç­‰å¾…æ™‚é–“ï¼Œé è¨­ 30 ç§’
#     - byï¼šé è¨­ By.NAME

# In[9]:


def select_ISBN_strategy(driver, select_position, option_position, waiting_time=30, by=By.NAME):
    time.sleep(0.5)
    search_field = WebDriverWait(driver, waiting_time).until(EC.presence_of_element_located((by, select_position)))
    select = Select(search_field)
    select.select_by_value(option_position)


# ## search_ISBN(driver, ISBN, input_position, waiting_time=10, by=By.NAME)
# - ç”¨æ³•ï¼š
#     - ç­‰å¾… input å‡ºç¾ï¼Œè¼¸å…¥ ISBN ä¸¦æŒ‰ä¸‹ ENTER
# - åƒæ•¸ï¼š
#     - ISBNï¼šISBN
#     - input_positionï¼šinput ä½ç½®ï¼Œé è¨­ name
#     - waiting_timeï¼šç­‰å¾…æ™‚é–“ï¼Œé è¨­ 10 ç§’
#     - byï¼šé è¨­ By.NAME

# In[10]:


def search_ISBN(driver, ISBN, input_position, waiting_time=10, by=By.NAME):
    time.sleep(0.5)
    search_input = WebDriverWait(driver, waiting_time).until(EC.presence_of_element_located((by, input_position)))
    search_input.send_keys(ISBN)
    search_input.send_keys(Keys.ENTER)


# # å·²å®Œæˆçš„çˆ¬èŸ²ç¨‹å¼

# ## <mark>å®Œæˆ</mark>webpac_gov_crawler(driver, org, org_url, ISBN)
# - ã€æœ€å¾Œç·¨è¼¯ã€ï¼š2021/08/02
# - ã€å‡½å¼å®Œæˆåº¦ã€ï¼šæ¥µé«˜

# ### å‡½å¼èªªæ˜
# 
# - ã€é‹ä½œçš„åŸç†ã€ï¼š
# - ã€é©ç”¨çš„æ©Ÿæ§‹ã€ï¼š[å®œè˜­ç¸£å…¬å…±åœ–æ›¸é¤¨](https://webpac.ilccb.gov.tw/)ã€[æ¡ƒåœ’å¸‚ç«‹åœ–æ›¸é¤¨](https://webpac.typl.gov.tw/)ã€[é«˜é›„å¸‚ç«‹åœ–æ›¸é¤¨](https://webpacx.ksml.edu.tw/)ã€[å±æ±ç¸£å…¬å…±åœ–æ›¸é¤¨](https://library.pthg.gov.tw/)ã€[èŠ±è“®ç¸£å…¬å…±åœ–æ›¸é¤¨](https://center.hccc.gov.tw/)ã€[æ¾æ¹–ç¸£å…¬å…±åœ–æ›¸é¤¨](https://webpac.phlib.nat.gov.tw/)ã€[åœ‹ç«‹é›²æ—ç§‘æŠ€å¤§å­¸](https://www.libwebpac.yuntech.edu.tw/)ã€[åœ‹å®¶é›»å½±åŠè¦–è½æ–‡åŒ–ä¸­å¿ƒ](https://lib.tfi.org.tw/)
# - ã€èƒ½è™•ç†ç‹€æ³ã€ï¼šåˆ¤æ–·æœå°‹çµæœæœ‰æ²’æœ‰è¶…éä¸€ç­†ã€åªæœ‰ä¸€ç­†æœå°‹çµæœæœ‰æ²’æœ‰è·³è½‰ã€[å¤šç­†](https://webpac.typl.gov.tw/search?searchField=ISBN&searchInput=986729193X)ã€æ‰¾ä¸åˆ°æ›¸ã€[ä¸æ–·çš„é»æ“Šï¼‚è¼‰å…¥æ›´å¤šï¼‚](https://webpac.ilccb.gov.tw/bookDetail/419482?qs=%7B%5Eurl3%2C%2Fsearch4%2Cquery%5E%3A%7B%5Ephonetic3%2C04%2CqueryType3%2C04%2C%2Cs23%2CISBN4%2C%2Cs13%2C9789573317241%5E%7D%7D)
# - ã€ä¸‹ä¸€æ­¥å„ªåŒ–ã€ï¼š

# ### å‡½å¼æœ¬é«”

# In[11]:


def click_more_btn(driver):
    try:
        while True:
            more_btn = wait_for_element_clickable(driver, 'è¼‰å…¥æ›´å¤š')
            if not more_btn:
                return
            more_btn.click()
            time.sleep(2)  # ä¸å¾—å·²çš„å¼·åˆ¶ç­‰å¾…
    except:
        return


# In[12]:


def webpac_gov_crawler(driver, org, org_url, ISBN):
    try:
        table = []

        driver.get(org_url + 'advanceSearch')
        select_ISBN_strategy(driver, 'searchField', 'ISBN')
        search_ISBN(driver, ISBN, 'searchInput')

        # ä¸€ç­†
        if wait_for_element_present(driver, '.bookplace_list > table', 10):
            click_more_btn(driver)
            tgt = accurately_find_table_and_read_it(driver, '.bookplace_list > table')
            tgt['åœ–æ›¸é¤¨'], tgt['é€£çµ'] = org, driver.current_url
            table.append(tgt)
        # å¤šç­†
        elif wait_for_element_present(driver, '.data_all .data_quantity2 em', 5):
            # å–å¾—å¤šå€‹é€£çµ
            tgt_urls = []
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            anchors = soup.select('.bookdata > h2 > a')
            for anchor in anchors:
                tgt_urls.append(org_url + anchor['href'])
            # é€²å…¥ä¸åŒçš„é€£çµ
            for tgt_url in tgt_urls:
                driver.get(tgt_url)
                if wait_for_element_present(driver, '.bookplace_list > table', 10):
                    click_more_btn(driver)
                    tgt = accurately_find_table_and_read_it(driver, '.bookplace_list > table')
                    tgt['åœ–æ›¸é¤¨'], tgt['é€£çµ'] = org, driver.current_url
                    table.append(tgt)
        # ç„¡
        else:
            print(f'åœ¨ã€Œ{org}ã€æ‰¾ä¸åˆ°ã€Œ{ISBN}ã€')
            return

        table = organize_columns(table)
    except:
        print(f'åœ¨ã€Œ{org}ã€æœå°‹ã€Œ{ISBN}ã€æ™‚ï¼Œç™¼ç”Ÿä¸æ˜éŒ¯èª¤ï¼')
        return
    else:
        return table


# ## <mark>å®Œæˆ</mark>webpac_jsp_crawler(driver, org, org_url, ISBN)
# - ã€æœ€å¾Œç·¨è¼¯ã€ï¼š2021/08/02
# - ã€å‡½å¼å®Œæˆåº¦ã€ï¼šæ¥µé«˜

# ### å‡½å¼èªªæ˜
# - ã€é‹ä½œçš„åŸç†ã€ï¼š
#     - ä½¿ç”¨ selenium é€²è¡Œæœç´¢ã€‚
#     - å¤§é‡ä½¿ç”¨ wait æ©Ÿåˆ¶ï¼Œä¾†æ‡‰å°åŠ è¼‰éæ…¢çš„ç¶²é ï¼ˆä¾‹ï¼š[ä½›å…‰å¤§å­¸](http://libils.fgu.edu.tw/webpacIndex.jsp)ï¼‰
#     - ç•¶æœå°‹çµæœåªæœ‰ä¸€ç­†æ™‚ï¼Œæœ‰äº›ç¶²ç«™æœƒç›´æ¥é€²å…¥ï¼‚æ›¸ç›®è³‡æ–™ï¼‚ï¼ˆä¾‹ï¼š[åœ‹ç«‹å®œè˜­å¤§å­¸](https://lib.niu.edu.tw/webpacIndex.jsp)ï¼‰
#         - é‚„æ˜¯æœƒåœç•™åœ¨ï¼‚æœå°‹çµæœï¼‚é é¢ï¼Œä½†å¤§éƒ¨åˆ†æœƒçœ‹ä¸åˆ°ï¼Œç¶²å€ä»æœƒæ”¹è®Šï¼Œæ‰€ä»¥ç„¡æ³•ç”¨ç¶²å€åˆ¤å®š
#     - ç•¶æœå°‹çµæœæœ‰å¤šç­†æ™‚ï¼Œæœƒè¦åˆ‡æ›åˆ° iframe çˆ¬å–ã€‚
#     - æœ‰äº›ï¼‚æ›¸ç›®è³‡æ–™ï¼‚æœƒæœ‰æ²’æœ‰è¡¨æ ¼çš„æƒ…æ³ï¼ˆä¾‹ï¼š[ä¸­è¯ç§‘å¤§](http://192.192.231.232/bookDetail.do?id=260965&nowid=3&resid=188809854)ï¼‰
# - ã€é©ç”¨çš„æ©Ÿæ§‹ã€ï¼š[è‡ºåŒ—å¸‚ç«‹åœ–æ›¸é¤¨](https://book.tpml.edu.tw/webpac/webpacIndex.jsp)ã€[åœ‹ç«‹å®œè˜­å¤§å­¸](https://lib.niu.edu.tw/webpacIndex.jsp)ã€[ä½›å…‰å¤§å­¸](http://libils.fgu.edu.tw/webpacIndex.jsp)ã€[å˜‰å—è—¥ç†å¤§å­¸](https://webpac.cnu.edu.tw/webpacIndex.jsp)ã€â€¦â€¦
# - ã€èƒ½è™•ç†ç‹€æ³ã€ï¼š[ä¸€ç­†](http://webpac.meiho.edu.tw/bookDetail.do?id=194508)ã€[ç„¡](http://webpac.meiho.edu.tw/bookSearchList.do?searchtype=simplesearch&search_field=ISBN&search_input=97895733172411&searchsymbol=hyLibCore.webpac.search.common_symbol&execodehidden=true&execode=&ebook=)ã€[å¤šç­†](http://webpac.meiho.edu.tw/bookSearchList.do?searchtype=simplesearch&execodeHidden=true&execode=&search_field=ISBN&search_input=9789573317241&searchsymbol=hyLibCore.webpac.search.common_symbol&resid=189006169&nowpage=1#searchtype=simplesearch&execodeHidden=true&execode=&search_field=ISBN&search_input=9789573317241&searchsymbol=hyLibCore.webpac.search.common_symbol&resid=189006169&nowpage=1)ã€[ç„¡è¡¨æ ¼](http://192.192.231.232/bookDetail.do?id=260965&nowid=3&resid=188809854)
# - ã€ä¸‹ä¸€æ­¥å„ªåŒ–ã€ï¼š
#     - çµ±ä¸€ search_input.submit() å’Œ search_input.send_keys(Keys.ENTER)ï¼Ÿ

# ### å‡½å¼æœ¬é«”

# In[13]:


def webpac_jsp_crawler(driver, org, org_url, ISBN):
    try:
        table = []
        
        driver.get(org_url)
        try:
            select_ISBN_strategy(driver, 'search_field', 'ISBN')
        except:
            select_ISBN_strategy(driver, 'search_field', 'STANDARDNO')  # åŒ—ç§‘å¤§
        search_ISBN(driver, ISBN, 'search_input')
        
        # ä¸€ç­†
        if wait_for_element_present(driver, 'div.mainCon'):
            if not wait_for_element_present(driver, 'table.order'):
                print(f'åœ¨ã€Œ{org}ã€æ‰¾ä¸åˆ°ã€Œ{ISBN}ã€')
                return
            tgt = accurately_find_table_and_read_it(driver, 'table.order')
            tgt['åœ–æ›¸é¤¨'], tgt['é€£çµ'] = org, driver.current_url
            table.append(tgt)
        # å¤šç­†ã€é›¶ç­†
        elif wait_for_element_present(driver, 'iframe#leftFrame'):
            iframe = driver.find_element_by_id('leftFrame')
            driver.switch_to.frame(iframe)
            time.sleep(1)  # åˆ‡æ›åˆ° <frame> éœ€è¦æ™‚é–“ï¼Œå¦å‰‡æœƒç„¡æ³•è®€å–
            
            # åˆ¤æ–·æ˜¯ä¸æ˜¯ï¼‚é›¶ç­†ï¼‚æŸ¥è©¢çµæœ
            if wait_for_element_present(driver, '#totalpage').text == '0':
                print(f'åœ¨ã€Œ{org}ã€æ‰¾ä¸åˆ°ã€Œ{ISBN}ã€')
                return
            
            # ï¼‚å¤šç­†ï¼‚æŸ¥è©¢çµæœ
            tgt_urls = []
            anchors = driver.find_elements(By.LINK_TEXT, 'è©³ç´°å…§å®¹')
            if anchors == []:
                anchors = driver.find_elements(By.LINK_TEXT, 'å…§å®¹')
            for anchor in anchors:
                tgt_urls.append(anchor.get_attribute('href'))

            for tgt_url in tgt_urls:
                driver.get(tgt_url)
                # ç­‰å¾…å…ƒç´ å‡ºç¾ï¼Œå¦‚æœå‡ºç¾ï¼Œé‚£éº¼æŠ“å– DataFrameï¼›å¦‚æœæ²’å‡ºç¾ï¼Œé‚£éº¼è·³å‡ºè¿´åœˆ
                if not wait_for_element_present(driver, 'table.order'):
                    continue  # æš«åœï¼‚æœ¬æ¬¡ï¼‚è¿´åœˆï¼Œä»¥ä¸‹æ•˜è¿°ä¸æœƒåŸ·è¡Œ
                tgt = accurately_find_table_and_read_it(driver, 'table.order')
                tgt['åœ–æ›¸é¤¨'], tgt['é€£çµ'] = org, driver.current_url
                table.append(tgt)
        table = organize_columns(table)
    except Exception as e:
        print(f'åœ¨ã€Œ{org}ã€æœå°‹ã€Œ{ISBN}ã€æ™‚ï¼Œç™¼ç”ŸéŒ¯èª¤ï¼ŒéŒ¯èª¤è¨Šæ¯ç‚ºï¼šã€Œ{e}ã€ï¼')
        return
    else:
        return table


# ## <mark>å®Œæˆ</mark>easy_crawler(driver, org, org_url, ISBN)
# - ã€æœ€å¾Œç·¨è¼¯ã€ï¼š2021/08/02
# - ã€å‡½å¼å®Œæˆåº¦ã€ï¼šæ¥µé«˜

# ### å‡½å¼èªªæ˜
# - ã€é‹ä½œçš„åŸç†ã€ï¼šå¾…è¼¸å…¥
# - ã€é©ç”¨çš„æ©Ÿæ§‹ã€ï¼š[åœ‹ç«‹è‡ºç£å¸«ç¯„å¤§å­¸](https://opac.lib.ntnu.edu.tw/search*cht/i)ã€[åœ‹ç«‹è‡ºç£ç§‘æŠ€å¤§å­¸](https://sierra.lib.ntust.edu.tw/search*cht/i)ã€[åœ‹ç«‹è‡ºç£æµ·æ´‹å¤§å­¸](https://ocean.ntou.edu.tw/search*cht/i)ã€[ä¸­åŸå¤§å­¸](http://cylis.lib.cycu.edu.tw/search*cht/i)ã€[é€¢ç”²å¤§å­¸](https://innopac.lib.fcu.edu.tw/search*cht/i)ã€[æœé™½ç§‘æŠ€å¤§å­¸](https://millennium.lib.cyut.edu.tw/search*cht/i)ã€[åœ‹ç«‹ä¸­å±±å¤§å­¸](https://dec.lib.nsysu.edu.tw/search*cht/i)ã€[åœ‹ç«‹é«˜é›„å¸«ç¯„å¤§å­¸](https://nknulib.nknu.edu.tw/search*cht/i)ã€[æ–‡è—»å¤–èªå¤§å­¸](https://libpac.wzu.edu.tw/search*cht/i)ã€[å¤§ä»ç§‘æŠ€å¤§å­¸](http://lib.tajen.edu.tw/search*cht/i)ã€[åœ‹ç«‹ä¸­å¤®å¤§å­¸](https://opac.lib.ncu.edu.tw/search*cht/i)
# - ã€èƒ½è™•ç†ç‹€æ³ã€ï¼šä¸€ç­†ã€ç„¡
# - ã€ä¸‹ä¸€æ­¥å„ªåŒ–ã€ï¼š
#     - å¾…è¼¸å…¥
#     - å¾…è¼¸å…¥

# ### å‡½å¼æœ¬é«”

# In[14]:


def easy_crawler(driver, org, org_url, ISBN):
    try:
        driver.get(org_url)
        search_ISBN(driver, ISBN, 'SEARCH')

        if not wait_for_element_present(driver, 'table.bibItems'):
            print(f'åœ¨ã€Œ{org}ã€æ‰¾ä¸åˆ°ã€Œ{ISBN}ã€')
            return

        table = accurately_find_table_and_read_it(driver, 'table.bibItems')
        table['åœ–æ›¸é¤¨'], table['é€£çµ'] = org, driver.current_url
        table = organize_columns(table)
    except Exception as e:
        print(f'åœ¨ã€Œ{org}ã€æœå°‹ã€Œ{ISBN}ã€æ™‚ï¼Œç™¼ç”ŸéŒ¯èª¤ï¼ŒéŒ¯èª¤è¨Šæ¯ç‚ºï¼šã€Œ{e}ã€ï¼')
        return
    else:
        return table


# ## <mark>å®Œæˆ</mark>webpac_pro_crawler(driver, org, org_url, ISBN)
# - ã€æœ€å¾Œç·¨è¼¯ã€ï¼š2021/08/02
# - ã€å‡½å¼å®Œæˆåº¦ã€ï¼šæ¥µé«˜

# ### å‡½å¼èªªæ˜
# - ã€é‹ä½œçš„åŸç†ã€ï¼šå¾…è¼¸å…¥
# - ã€é©ç”¨çš„æ©Ÿæ§‹ã€ï¼š[ä¸­å¤®ç ”ç©¶é™¢](https://las.sinica.edu.tw/*cht)ã€[ä¸­åœ‹æ–‡åŒ–å¤§å­¸](https://webpac.pccu.edu.tw/*cht)ã€[è¼”ä»å¤§å­¸](https://library.lib.fju.edu.tw/)ã€[åœ‹ç«‹é™½æ˜äº¤é€šå¤§å­¸](https://library.ym.edu.tw/screens/opacmenu_cht_s7.html)
# - ã€èƒ½è™•ç†ç‹€æ³ã€ï¼šä¸€ç­†ã€ç„¡
# - ã€ä¸‹ä¸€æ­¥å„ªåŒ–ã€ï¼š
#     - å¾…è¼¸å…¥
#     - å¾…è¼¸å…¥

# ### å‡½å¼æœ¬é«”

# In[15]:


def webpac_pro_crawler(driver, org, org_url, ISBN):
    try:
        driver.get(org_url)
        select_ISBN_strategy(driver, 'searchtype', 'i')
        search_ISBN(driver, ISBN, 'searcharg')

        if not wait_for_element_present(driver, 'table.bibItems'):
            print(f'åœ¨ã€Œ{org}ã€æ‰¾ä¸åˆ°ã€Œ{ISBN}ã€')
            return

        table = accurately_find_table_and_read_it(driver, 'table.bibItems')
        table['åœ–æ›¸é¤¨'], table['é€£çµ'] = org, driver.current_url
        table = organize_columns(table)
    except Exception as e:
        print(f'åœ¨ã€Œ{org}ã€æœå°‹ã€Œ{ISBN}ã€æ™‚ï¼Œç™¼ç”ŸéŒ¯èª¤ï¼ŒéŒ¯èª¤è¨Šæ¯ç‚ºï¼šã€Œ{e}ã€ï¼')
        return
    else:
        return table


# ## <mark>å®Œæˆ</mark>webpac_ajax_crawler(driver, org, org_url, ISBN)
# - ã€æœ€å¾Œç·¨è¼¯ã€ï¼š2021/08/02
# - ã€å‡½å¼å®Œæˆåº¦ã€ï¼šæ¥µé«˜

# ### å‡½å¼èªªæ˜
# - ã€é‹ä½œçš„åŸç†ã€ï¼šä½¿ç”¨ selenium é€²è¡Œæœç´¢ï¼Œé€²å…¥ï¼‚æ›¸ç›®è³‡æ–™ï¼‚é é¢å¾Œï¼Œå¾è©²ç¶²å€åˆ†æä¸¦å¾—åˆ° midï¼Œåœ¨ç”±æ­¤é€²å…¥ ajax_pageã€‚
# - ã€é©ç”¨çš„æ©Ÿæ§‹ã€ï¼š[æ–°åŒ—å¸‚ç«‹åœ–æ›¸é¤¨](https://webpac.tphcc.gov.tw/webpac/search.cfm)ã€[é«˜é›„å¸‚ç«‹ç©ºä¸­å¤§å­¸](https://webpac.ouk.edu.tw/webpac/search.cfm)ã€[åœ‹ç«‹å±æ±å¤§å­¸](https://webpac.nptu.edu.tw/webpac/search.cfm)
# - ã€èƒ½è™•ç†ç‹€æ³ã€ï¼šåˆ¤æ–·æœå°‹çµæœæœ‰æ²’æœ‰è¶…éä¸€ç­†ã€åªæœ‰ä¸€ç­†æœå°‹çµæœæœ‰æ²’æœ‰è·³è½‰ã€æ‰¾ä¸åˆ°æ›¸
# - ã€ä¸‹ä¸€æ­¥å„ªåŒ–ã€ï¼šç•¶æœå°‹ç„¡çµæœæ™‚ï¼Œå¯ä»¥ç›´æ¥çµæŸã€‚

# ### å‡½å¼æœ¬é«”

# In[16]:


def webpac_ajax_crawler(driver, org, org_url, ISBN):
    try:
        table = []

        driver.get(org_url)
        wait_for_element_clickable(driver, 'é€²éšæŸ¥è©¢').click()  # é»æ“Šï¼‚é€²éšæŸ¥è©¢ï¼‚
        select_ISBN_strategy(driver, 'as_type_1', 'i', by=By.ID)
        search_ISBN(driver, ISBN, 'as_keyword_1', by=By.ID)

        org_url = org_url.replace('/search.cfm', '')
        if wait_for_element_present(driver, 'è©³ç´°æ›¸ç›®', by=By.LINK_TEXT):
            tgt_urls = []
            anchors = driver.find_elements_by_link_text('è©³ç´°æ›¸ç›®')
            for anchor in anchors:
                tgt_urls.append(anchor.get_attribute('href'))

            for tgt_url in tgt_urls:
                mid = tgt_url.split('mid=')[-1].split('&')[0]
                ajax_page_url = f'{org_url}/ajax_page/get_content_area.cfm?mid={mid}&i_list_number=250&i_page=1&i_sory_by=1'
                tgt = pd.read_html(ajax_page_url, encoding='utf-8')[0]
                tgt['åœ–æ›¸é¤¨'], tgt['é€£çµ'] = org, tgt_url
                table.append(tgt)
        elif wait_for_element_present(driver, 'div.book-detail'):  # é«˜é›„å¸‚ç«‹ç©ºä¸­å¤§å­¸ã€åœ‹ç«‹å±æ±å¤§å­¸æ‰æœƒé‡åˆ°è·³è½‰
            tgt_url = driver.current_url
            mid = tgt_url.split('mid=')[-1].split('&')[0]
            ajax_page_url = f'{org_url}/ajax_page/get_content_area.cfm?mid={mid}&i_list_number=250&i_page=1&i_sory_by=1'
            tgt = pd.read_html(ajax_page_url, encoding='utf-8')[0]
            tgt['åœ–æ›¸é¤¨'], tgt['é€£çµ'] = org, tgt_url
            table.append(tgt)
        table = organize_columns(table)
    except Exception as e:
        print(f'åœ¨ã€Œ{org}ã€æœå°‹ã€Œ{ISBN}ã€æ™‚ï¼Œç™¼ç”ŸéŒ¯èª¤ï¼ŒéŒ¯èª¤è¨Šæ¯ç‚ºï¼šã€Œ{e}ã€ï¼')
        return
    else:
        return table


# ## <mark>å®Œæˆ</mark>webpac_aspx_crawler(driver, org, org_url, ISBN)
# - ã€æœ€å¾Œç·¨è¼¯ã€ï¼š2021/08/03
# - ã€å‡½å¼å®Œæˆåº¦ã€ï¼šé«˜

# ### å‡½å¼èªªæ˜
# - ã€é‹ä½œçš„åŸç†ã€ï¼šä¸€ç›´åˆ‡ iframe
# - ã€é©ç”¨çš„æ©Ÿæ§‹ã€ï¼š[æ¨¹å¾·ç§‘æŠ€å¤§å­¸](https://webpac.stu.edu.tw/webopac/)ã€[å°ç£é¦–åºœå¤§å­¸](http://120.114.1.19/webopac/Jycx.aspx?dc=1&fc=1&n=7)ã€[å´‘å±±ç§‘æŠ€å¤§å­¸](https://weblis.lib.ksu.edu.tw/webopac/)ã€ã€ã€
# - ã€èƒ½è™•ç†ç‹€æ³ã€ï¼šä¸€ç­†ã€å¤šç­†ã€ç„¡
# - ã€ä¸‹ä¸€æ­¥å„ªåŒ–ã€ï¼š
#     - ç„¡æ³•å–å¾—ï¼‚æ›¸ç›®è³‡æ–™ï¼‚çš„ç¶²å€ï¼Œç”¨çš„æ˜¯ JavaScript èªæ³•
#     - ugly code

# ### å‡½å¼æœ¬é«”

# In[17]:


def webpac_aspx_crawler(driver, org, org_url, ISBN):
    try:
        table = []

        driver.get(org_url)

        time.sleep(1.5)
        iframe = wait_for_element_present(driver, 'default', by=By.NAME)
        driver.switch_to.frame(iframe)
        select_ISBN_strategy(driver, 'ctl00$ContentPlaceHolder1$ListBox1', 'Info000076')
        search_ISBN(driver, ISBN, 'ctl00$ContentPlaceHolder1$TextBox1')
        driver.switch_to.default_content()
        
        i = 0
        while True:
            time.sleep(1.5)
            iframe = wait_for_element_present(driver, 'default', by=By.NAME)
            driver.switch_to.frame(iframe)
            try:
                wait_for_element_present(driver, f'//*[@id="ctl00_ContentPlaceHolder1_dg_ctl0{i+2}_lbtgcd2"]', by=By.XPATH).click()
            except:
                break
            driver.switch_to.default_content()

            time.sleep(1.5)
            iframe = wait_for_element_present(driver, 'default', by=By.NAME)
            driver.switch_to.frame(iframe)
            tgt = accurately_find_table_and_read_it(driver, '#ctl00_ContentPlaceHolder1_dg')
            tgt['åœ–æ›¸é¤¨'], tgt['é€£çµ'] = org, driver.current_url
            table.append(tgt)
            driver.switch_to.default_content()

            driver.back()
            i += 1

        try:
            table = organize_columns(table)
        except:
            print(f'åœ¨ã€Œ{org}ã€æ‰¾ä¸åˆ°ã€Œ{ISBN}ã€')
            return
    except Exception as e:
        print(f'åœ¨ã€Œ{org}ã€æœå°‹ã€Œ{ISBN}ã€æ™‚ï¼Œç™¼ç”ŸéŒ¯èª¤ï¼ŒéŒ¯èª¤è¨Šæ¯ç‚ºï¼šã€Œ{e}ã€ï¼')
        return
    else:
        return table


# In[18]:


# driver = webdriver.Chrome(options=my_options, desired_capabilities=my_capabilities)
# webpac_aspx_crawler(
#     driver=driver,
#     org='å¼˜å…‰ç§‘æŠ€å¤§å­¸',
#     org_url='https://webpac.hk.edu.tw/webopac/',
#     ISBN='9789869109321'
# )


# ## <mark>å®Œæˆ</mark>uhtbin_crawler(driver, org, org_url, ISBN)
# - ã€æœ€å¾Œç·¨è¼¯ã€ï¼š2021/08/03
# - ã€å‡½å¼å®Œæˆåº¦ã€ï¼šé«˜

# ### å‡½å¼èªªæ˜
# - ã€é‹ä½œçš„åŸç†ã€ï¼šå¾…è¼¸å…¥
# - ã€é©ç”¨çš„æ©Ÿæ§‹ã€ï¼š[åœ‹ç«‹è‡ºåŒ—è­·ç†å¥åº·å¤§å­¸](http://140.131.94.8/uhtbin/webcat)ã€[å¤§åŒå¤§å­¸](http://140.129.23.14/uhtbin/webcat)ã€[åœ‹ç«‹é«”è‚²å¤§å­¸](http://192.83.181.243/uhtbin/webcat)
# - ã€èƒ½è™•ç†ç‹€æ³ã€ï¼šä¸€ç­†ã€ç„¡
# - ã€ä¸‹ä¸€æ­¥å„ªåŒ–ã€ï¼š
#     - å¾…è¼¸å…¥
#     - å¾…è¼¸å…¥

# ### å‡½å¼æœ¬é«”

# In[19]:


def uhtbin_crawler(driver, org, org_url, ISBN):
    try:
        driver.get(org_url)
        try:
            select_ISBN_strategy(driver, 'srchfield1', 'GENERAL^SUBJECT^GENERAL^^æ‰€æœ‰æ¬„ä½')
        except:
            select_ISBN_strategy(driver, 'srchfield1', '020^SUBJECT^SERIES^Title Processing^ISBN')
        search_ISBN(driver, ISBN, 'searchdata1')
        
        if 'æœªåœ¨ä»»ä½•åœ–æ›¸é¤¨æ‰¾åˆ°' in driver.find_element(By.CSS_SELECTOR, 'table').text:
            print(f'åœ¨ã€Œ{org}ã€æ‰¾ä¸åˆ°ã€Œ{ISBN}ã€')
            return
        
        table = accurately_find_table_and_read_it(driver, 'table')
        
        # ç‰¹æ®Šè™•ç†
        table.drop([0, 1, 2], inplace=True)
        table.drop([1, 2, 4], axis='columns', inplace=True)
        table.rename(columns={0: 'ç´¢æ›¸è™Ÿ', 3: 'é¤¨è—ç‹€æ…‹'}, inplace=True)
        table['åœ–æ›¸é¤¨'], table['é€£çµ'], table['é¤¨è—åœ°'] = org, driver.current_url, table['é¤¨è—ç‹€æ…‹']
        
        table = organize_columns(table)
    except Exception as e:
        print(f'åœ¨ã€Œ{org}ã€æœå°‹ã€Œ{ISBN}ã€æ™‚ï¼Œç™¼ç”ŸéŒ¯èª¤ï¼ŒéŒ¯èª¤è¨Šæ¯ç‚ºï¼šã€Œ{e}ã€ï¼')
        return
    else:
        return table


# In[20]:


# driver = webdriver.Chrome(options=my_options, desired_capabilities=my_capabilities)
# uhtbin_crawler(
#     driver=driver,
#     org='å¤§åŒå¤§å­¸',
#     org_url='http://140.129.23.14/uhtbin/webcat',
#     ISBN='9789861371955'
# )


# ## <mark>å®Œæˆ</mark>toread_crawler(driver, org, org_url, ISBN)
# - ã€æœ€å¾Œç·¨è¼¯ã€ï¼š2021/08/05
# - ã€å‡½å¼å®Œæˆåº¦ã€ï¼šé«˜ã€çˆ†è¤‡é›œ

# ### å‡½å¼èªªæ˜
# - ã€é‹ä½œçš„åŸç†ã€ï¼šå¾…è¼¸å…¥
# - ã€é©ç”¨çš„æ©Ÿæ§‹ã€ï¼š[å½°åŒ–ç¸£åœ–æ›¸é¤¨](https://library.toread.bocach.gov.tw/toread/opac)ã€toread ç³»çµ±
# - ã€èƒ½è™•ç†ç‹€æ³ã€ï¼šä¸€ç­†ã€ç„¡ã€å¤šç­†ã€[ç¿»é ](https://library.toread.bocach.gov.tw/toread/opac/bibliographic_view?NewBookMode=false&id=341724&mps=10&q=986729193X+OR+9789867291936&start=0&view=CONTENT)
# - ã€ä¸‹ä¸€æ­¥å„ªåŒ–ã€ï¼š
#     - å¾…è¼¸å…¥

# ### å‡½å¼æœ¬é«”

# In[21]:


def toread_crawler(driver, org, org_url, ISBN):
    try:
        table = []

        driver.get(org_url)
        search_ISBN(driver, ISBN, 'q')

        if not wait_for_element_present(driver, 'div#results'):
            print(f'åœ¨{org}è£¡ï¼Œæ²’æœ‰ã€Š{ISBN}ã€‹')
            return

        # æœ‰ div#resultsï¼Œæ‰¾å‡ºæ‰€æœ‰çš„ï¼‚æ›¸ç›®è³‡æ–™ï¼‚çš„ç¶²å€
        tgt_urls = []
        anchors = driver.find_elements(By.CSS_SELECTOR, 'div.img_reslt > a')
        for anchor in anchors:
            tgt_urls.append(anchor.get_attribute('href'))

        # é€²å…¥å„å€‹ï¼‚æ›¸ç›®è³‡æ–™ï¼‚çˆ¬å–è¡¨æ ¼
        for tgt_url in tgt_urls:
            driver.get(tgt_url)
            
            # é›»å­æ›¸æ²’æœ‰ table
            if not wait_for_element_present(driver, 'table.gridTable'):
                continue

            tgt = accurately_find_table_and_read_it(driver, 'table.gridTable')
            tgt['åœ–æ›¸é¤¨'], tgt['é€£çµ'] = org, tgt_url

            # ä»¥ä¸‹å…©è¡Œï¼Œæ˜¯ï¼‚å½°åŒ–ç¸£å…¬å…±åœ–æ›¸é¤¨ï¼‚æœ‰å¤šé¤˜çš„ rowï¼Œé ˆè¦ç‰¹åˆ¥ç¯©é¸èª¿ NaN
            try:
                tgt = tgt.dropna(subset=['å…¸è—åœ°åç¨±'])
            except:  # åœ‹ç«‹é«˜é›„å¤§å­¸æ²’æœ‰é€™å€‹ç‹€æ³
                pass
            tgt.reset_index(drop=True, inplace=True)

            table.append(tgt)
            
            # æ›é ï¼šæ›¸æ²’æœ‰é‚£éº¼å¤šå§ XDï¼ŒåœŸæ³•ç…‰é‹¼æ³•
            i = 0
            while True:
                try:
                    wait_for_element_clickable(driver, str(2+i)).click()
                    time.sleep(2.5)
                    tgt = accurately_find_table_and_read_it(driver, 'table.gridTable')
                    tgt['åœ–æ›¸é¤¨'], tgt['é€£çµ'] = org, tgt_url

                    # ä»¥ä¸‹å…©è¡Œï¼Œæ˜¯ï¼‚å½°åŒ–ç¸£å…¬å…±åœ–æ›¸é¤¨ï¼‚æœ‰å¤šé¤˜çš„ rowï¼Œé ˆè¦ç‰¹åˆ¥ç¯©é¸èª¿ NaN
                    try:
                        tgt = tgt.dropna(subset=['å…¸è—åœ°åç¨±'])
                    except:  # åœ‹ç«‹é«˜é›„å¤§å­¸æ²’æœ‰é€™å€‹ç‹€æ³
                        pass
                    tgt.reset_index(drop=True, inplace=True)

                    table.append(tgt)
                    i += 1
                except:
                    break
        table = organize_columns(table)
    except Exception as e:
        print(f'åœ¨ã€Œ{org}ã€æœå°‹ã€Œ{ISBN}ã€æ™‚ï¼Œç™¼ç”ŸéŒ¯èª¤ï¼ŒéŒ¯èª¤è¨Šæ¯ç‚ºï¼šã€Œ{e}ã€ï¼')
        return
    else:
        return table


# In[22]:


# driver = webdriver.Chrome(options=my_options, desired_capabilities=my_capabilities)
# toread_crawler(
#     driver=driver,
#     org='é«˜é›„é†«å­¸å¤§å­¸',
#     org_url='https://toread.kmu.edu.tw/toread/opac',
#     ISBN='9789861371955'
# )


# ## <mark>å®Œæˆ</mark>é€£æ±Ÿç¸£å…¬å…±åœ–æ›¸é¤¨(driver, org, org_url, ISBN)
# - ã€æœ€å¾Œç·¨è¼¯ã€ï¼š2021/08/03
# - ã€å‡½å¼å®Œæˆåº¦ã€ï¼šæ¥µé«˜

# ### å‡½å¼èªªæ˜
# - ã€é‹ä½œçš„åŸç†ã€ï¼šå¾…è¼¸å…¥
# - ã€é©ç”¨çš„æ©Ÿæ§‹ã€ï¼š[é€£æ±Ÿç¸£å…¬å…±åœ–æ›¸é¤¨](http://210.63.206.76/Webpac2/msearch.dll/)ã€[é–‹å—å¤§å­¸](http://www.lib.knu.edu.tw/Webpac2/msearch.dll/)
# - ã€èƒ½è™•ç†ç‹€æ³ã€ï¼šä¸€ç­†ã€ç„¡
# - ã€ä¸‹ä¸€æ­¥å„ªåŒ–ã€ï¼š
#     - é–‹å—å¤§å­¸æœå°‹å“ˆåˆ©æ³¢ç‰¹æœƒæœ‰å¤šå€‹æƒ…æ³

# ### å‡½å¼æœ¬é«”

# In[23]:


def é€£æ±Ÿç¸£å…¬å…±åœ–æ›¸é¤¨(driver, org, org_url, ISBN):
    try:
        table = []

        driver.get(org_url)
        search_ISBN(driver, ISBN, 'ISBN')

        if wait_for_element_present(driver, 'é‡æ–°æŸ¥è©¢', by=By.LINK_TEXT):
            print(f'åœ¨ã€Œ{org}ã€æ‰¾ä¸åˆ°ã€Œ{ISBN}ã€')
            return

        tgt = accurately_find_table_and_read_it(driver, 'table', -2)
        tgt['åœ–æ›¸é¤¨'], tgt['é€£çµ'] = org, driver.current_url
        table.append(tgt)

        table = organize_columns(table)
    except Exception as e:
        print(f'åœ¨ã€Œ{org}ã€æœå°‹ã€Œ{ISBN}ã€æ™‚ï¼Œç™¼ç”ŸéŒ¯èª¤ï¼ŒéŒ¯èª¤è¨Šæ¯ç‚ºï¼šã€Œ{e}ã€ï¼')
        return
    else:
        return table


# # è‡ªæˆ‘ç¨ç«‹çš„çˆ¬èŸ²ç¨‹å¼

# ## <mark>å®Œæˆ</mark>åœ‹å®¶åœ–æ›¸é¤¨(driver, org, org_url, ISBN)
# - ã€æœ€å¾Œç·¨è¼¯ã€ï¼š2021/08/02
# - ã€å‡½å¼å®Œæˆåº¦ã€ï¼šæ¥µé«˜

# ### å‡½å¼èªªæ˜
# - ã€é‹ä½œçš„åŸç†ã€ï¼šä½¿ç”¨ Selenium
# - ã€é©ç”¨çš„æ©Ÿæ§‹ã€ï¼š[åœ‹å®¶åœ–æ›¸é¤¨](https://aleweb.ncl.edu.tw/F)
# - ã€èƒ½è™•ç†ç‹€æ³ã€ï¼šæ‰¾ä¸åˆ°ã€ä¸€ç­†ã€[ç„¡è¡¨æ ¼å…§å®¹](https://aleweb.ncl.edu.tw/F/MPXYG72FRS6Q4T31JTU5GKITQSE7B3ASA51D88R8BSTBT6T6E5-03970?func=item-global&doc_library=TOP02&doc_number=003632992&year=&volume=&sub_library=)
# - ã€ä¸‹ä¸€æ­¥å„ªåŒ–ã€ï¼š
#     - 9789861371955
#     - ç›®å‰å°šæœªé‡åˆ°å¤šç­†æƒ…æ³
#     - ä¸çŸ¥é“å¯ä»¥å’Œä»€éº¼æ©Ÿæ§‹çš„ç³»çµ±åˆä½µåœ¨ä¸€èµ·ï¼Ÿ

# ### å‡½å¼æœ¬é«”

# In[24]:


def åœ‹å®¶åœ–æ›¸é¤¨(driver, org, org_url, ISBN):
    try:
        driver.get(org_url)
        select_ISBN_strategy(driver, 'find_code', 'ISBN')
        search_ISBN(driver, ISBN, 'request')

        # é»æ“Šï¼‚æ›¸åœ¨å“ªè£¡(è«‹é»é¸)ï¼‚ï¼Œé€²å…¥ï¼‚æ›¸ç›®è³‡æ–™ï¼‚
        wait_for_element_clickable(driver, 'æ›¸åœ¨å“ªè£¡(è«‹é»é¸)').click()

        table = accurately_find_table_and_read_it(driver, 'table', -2)
        if 0 in table.columns:
            print(f'åœ¨ã€Œ{org}ã€æ‰¾ä¸åˆ°ã€Œ{ISBN}ã€')
            return
        table['åœ–æ›¸é¤¨'], table['é€£çµ'] = org, driver.current_url
        table = organize_columns(table)
    except Exception as e:
        # æ²’æœ‰ç‰©ä»¶å¯ä»¥ clickï¼Œè¡¨ç¤ºï¼‚é›¶ç­†ï¼‚æœå°‹çµæœ
        print(f'åœ¨ã€Œ{org}ã€æœå°‹ã€Œ{ISBN}ã€æ™‚ï¼Œç™¼ç”ŸéŒ¯èª¤ï¼ŒéŒ¯èª¤è¨Šæ¯ç‚ºï¼šã€Œ{e}ã€ï¼')
        return
    return table


# ## <mark>å®Œæˆ</mark>ä¸–æ–°å¤§å­¸(driver, org, org_url, ISBN)
# - ã€æœ€å¾Œç·¨è¼¯ã€ï¼š2021/08/03
# - ã€å‡½å¼å®Œæˆåº¦ã€ï¼šæ¥µé«˜

# ### å‡½å¼èªªæ˜
# - ã€é‹ä½œçš„åŸç†ã€ï¼šå¾…è¼¸å…¥
# - ã€é©ç”¨çš„æ©Ÿæ§‹ã€ï¼š[ä¸–æ–°å¤§å­¸](https://koha.shu.edu.tw/)
# - ã€èƒ½è™•ç†ç‹€æ³ã€ï¼šä¸€ç­†ã€ç„¡
# - ã€ä¸‹ä¸€æ­¥å„ªåŒ–ã€ï¼š
#     - å¾…è¼¸å…¥
#     - å¾…è¼¸å…¥

# ### å‡½å¼æœ¬é«”

# In[25]:


def ä¸–æ–°å¤§å­¸(driver, org, org_url, ISBN):
    try:
        driver.get(org_url)
        search_ISBN(driver, ISBN, 'request')

        table = accurately_find_table_and_read_it(driver, '#holdingst')
        table['åœ–æ›¸é¤¨'], table['é€£çµ'] = org, driver.current_url
        table = organize_columns(table)
    except Exception as e:
        print(f'åœ¨ã€Œ{org}ã€æ‰¾ä¸åˆ°ã€Œ{ISBN}ã€')
        return
    else:
        return table


# In[26]:


# driver = webdriver.Chrome(options=my_options, desired_capabilities=my_capabilities)
# ä¸–æ–°å¤§å­¸(
#     driver=driver,
#     org='ä¸–æ–°å¤§å­¸',
#     org_url='https://koha.shu.edu.tw/',
#     ISBN='9789573317241'
# )


# ## åœ‹ç«‹è‡ºç£åšç‰©é¤¨

# In[27]:


# ISBN = 9789865321703  # å°‘ç”·å°‘å¥³è¦‹å­¸ä¸­ : æ—¥æœ¬æ™‚ä»£ä¿®å­¸æ—…è¡Œé–‹ç®±
# org_url = 'https://lib.moc.gov.tw/F'

# my_options = Options()
# my_options.add_argument("--incognito")  # é–‹å•Ÿç„¡ç—•æ¨¡å¼
# # my_options.add_argument("--headless")  # ä¸é–‹å•Ÿå¯¦é«”ç€è¦½å™¨
# driver = webdriver.Chrome(options=my_options)
# driver.get("")

# time.sleep(1)  # ç‚ºäº†ç­‰å¾…ç¶²é åŠ è¼‰
# select = Select(driver.find_element_by_name("x"))
# select.select_by_visible_text(u"ISBN")

# search_input = driver.find_element_by_name("y")
# search_input.send_keys(ISBN)
# # search_input.submit()  # ä¸çŸ¥é“ç‚ºä»€éº¼ç„¡æ³• submit()ï¼Ÿ
# submit_input = driver.find_element_by_name("Search")
# submit_input.click()

# click = driver.find_element_by_xpath("/html/body/table[9]/tbody/tr/td[1]/table/tbody/tr[1]/td[2]/a")
# click.click()

# html_text = driver.page_source
# dfs = pd.read_html(html_text, encoding="utf-8")
# df_ntm = dfs[11]
# df_ntm


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# # é–å¦¤çš„çˆ¬èŸ²ç¨‹å¼

# ## <mark>å®Œæˆ</mark>webpac_two_cralwer(driver, org, org_url, ISBN)
# - ã€æœ€å¾Œç·¨è¼¯ã€ï¼š2021/08/05
# - ã€ç·¨è¼¯è€…ã€ï¼šé–å¦¤ã€ä»•ç‘‹
# - ã€é‹ç”¨çš„æ©Ÿæ§‹ã€ï¼š[åœ‹ç«‹è‡ºåŒ—è—è¡“å¤§å­¸](http://203.64.5.158/webpac/)ã€[åœ‹ç«‹å‹¤ç›Šç§‘æŠ€å¤§å­¸](http://140.128.95.172/webpac/)ã€[ç¾©å®ˆå¤§å­¸](http://webpac.isu.edu.tw/webpac/)ã€[ä¸­å±±é†«å­¸å¤§å­¸](http://140.128.138.208/webpac/)

# ### å‡½å¼æœ¬é«”

# In[41]:


def webpac_two_cralwer(driver, org, org_url, ISBN):
    try:
        tgt_url = f'{org_url}search/?q={ISBN}&field=isn&op=AND&type='
        driver.get(tgt_url)
        
        wait_for_element_clickable(driver, '/html/body/div/div[1]/div[2]/div/div/div[2]/div[3]/div[1]/div[3]/div/ul/li/div/div[2]/h3/a', waiting_time=15, by=By.XPATH).click()
        
        table = accurately_find_table_and_read_it(driver, '#LocalHolding > table')
        table['åœ–æ›¸é¤¨'], table['é€£çµ'] = org, driver.current_url
        
        # ç‰¹æ®Šç‹€æ³ï¼šåœ‹å®¶è¡›ç”Ÿç ”ç©¶é™¢
        if 'http://webpac.nhri.edu.tw/webpac/' in org_url:
            table.rename(columns={'é¤¨è—ç‹€æ…‹': 'wow', 'ç‹€æ…‹ï¼åˆ°æœŸæ—¥': 'é¤¨è—ç‹€æ…‹'}, inplace=True)
        
        table = organize_columns(table)
    except:
        print(f'åœ¨ã€Œ{org}ã€æ‰¾ä¸åˆ°ã€Œ{ISBN}ã€')
        return
    else:
        return table


# In[42]:


driver = webdriver.Chrome(options=my_options, desired_capabilities=my_capabilities)
webpac_two_cralwer(
    driver=driver,
    org='åœ‹å®¶è¡›ç”Ÿç ”ç©¶é™¢',
    org_url='http://webpac.nhri.edu.tw/webpac/',
    ISBN='9789861371955'
)


# In[30]:


# driver = webdriver.Chrome(options=my_options, desired_capabilities=my_capabilities)
# webpac_two_cralwer(
#     driver=driver,
#     org='åœ‹ç«‹è‡ºåŒ—è—è¡“å¤§å­¸',
#     org_url='http://203.64.5.158/webpac/',
#     ISBN='9789861371955'
# )


# ## <mark>å®Œæˆ</mark>å°åŒ—æµ·æ´‹ç§‘æŠ€å¤§å­¸(driver, org, org_url, ISBN)
# - ã€æœ€å¾Œç·¨è¼¯ã€ï¼š2021/08/03
# - ã€ç·¨è¼¯è€…ã€ï¼šé–å¦¤
# - ã€é‹ç”¨çš„æ©Ÿæ§‹ã€ï¼š[å°åŒ—æµ·æ´‹ç§‘æŠ€å¤§å­¸](http://140.129.253.4/webopac7/sim_data2.php?pagerows=15&orderby=BRN&pageno=1&bn=986729193X)

# In[31]:


def å°åŒ—æµ·æ´‹ç§‘æŠ€å¤§å­¸(driver, org, org_url, ISBN):
    try:
        df_lst = []
        org_url = org_url + ISBN
        driver.get(org_url)
        result = driver.find_element_by_id("qresult-content")
        trlist = result.find_elements_by_tag_name('tr')
        for row in range(2, len(trlist)):
            css = "#qresult-content > tbody > tr:nth-child(" + str(row) + ") > td:nth-child(3) > a"
            into = driver.find_element_by_css_selector(css).click()
            time.sleep(2)
            html_text = driver.page_source
            dfs = pd.read_html(html_text, encoding="utf-8")
            df_tumt = dfs[6]
            df_tumt.rename(columns={1: "é¤¨è—åœ°", 3: "ç´¢æ›¸è™Ÿ", 4: "é¤¨è—ç‹€æ…‹"}, inplace=True)
            df_tumt.drop([0], inplace=True)
            df_tumt["åœ–æ›¸é¤¨"], df_tumt["é€£çµ"] = "å°åŒ—æµ·æ´‹ç§‘æŠ€å¤§å­¸", driver.current_url
            df_tumt = organize_columns(df_tumt)
            df_lst.append(df_tumt)
            back = driver.find_element_by_css_selector("#table1 > tbody > tr > td:nth-child(1) > a:nth-child(3)").click()
        table = pd.concat(df_lst, axis=0, ignore_index=True)
    except Exception as e:
            print(f'åœ¨ã€Œ{org}ã€æœå°‹ã€Œ{ISBN}ã€æ™‚ï¼Œç™¼ç”ŸéŒ¯èª¤ï¼ŒéŒ¯èª¤è¨Šæ¯ç‚ºï¼šã€Œ{e}ã€ï¼')
            return
    else:
        return table


# In[32]:


# driver = webdriver.Chrome(options=my_options, desired_capabilities=my_capabilities)
# å°åŒ—æµ·æ´‹ç§‘æŠ€å¤§å­¸(
#     driver=driver,
#     org='å°åŒ—æµ·æ´‹ç§‘æŠ€å¤§å­¸',
#     org_url='http://140.129.253.4/webopac7/sim_data2.php?pageno=1&pagerows=15&orderby=BRN&ti=&au=&se=&su=&pr=&mt=&mt2=&yrs=&yre=&nn=&lc=&bn=',
#     ISBN='986729193X'
# )


# ## <font color='red'>é€²è¡Œä¸­</font>primo_crawler(driver, org, url_front, ISBN ,url_behind , tcn)

# - ã€æœ€å¾Œç·¨è¼¯ã€ï¼š2021/08/03
# - ã€ç·¨è¼¯è€…ã€ï¼šé–å¦¤
# - ã€é‹ç”¨çš„æ©Ÿæ§‹ã€ï¼š[åœ‹ç«‹è‡ºç£å¤§å­¸](https://ntu.primo.exlibrisgroup.com/discovery/search?sortby=rank&vid=886NTU_INST:886NTU_INST&lang=zh-tw)

# In[33]:


#å°å¤§ã€æ”¿å¤§ã€æ·¡æ±Ÿã€æ±å³ã€ç„¶å¾ŒéŠ˜å‚³æ²’æœ‰ç´¢æ›¸ç¢¼QQ(è¦å¦å¤–é€²å»ä½†æˆ‘æ‡¶ğŸ™„)
def primo_finding(driver, org, tcn): #primoçˆ¬è³‡è¨Šçš„def ï¼›#tcn = thelist_class_name
    sub_df_lst = []
    time.sleep(5)
    try:
        back = driver.find_element_by_css_selector(".tab-header .back-button.button-with-icon.zero-margin.md-button.md-primoExplore-theme.md-ink-ripple")
    except:
        back = None
    if back != None:
        back.click()

    thelist = driver.find_elements_by_class_name(tcn)
    if tcn == 'md-2-line.md-no-proxy._md': #å¦‚æœæ˜¯æ±å³æˆ–éŠ˜å‚³
        thelist = thelist[0:-2]
    else:
        pass

    for row in thelist:
        plist = row.find_elements_by_tag_name("p")
        where = row.find_elements_by_tag_name("h3")
        i = len(where)
        for sth in plist:
            a = sth.find_elements_by_tag_name("span")
            for _ in range(i):
                now_url = driver.current_url
                new_row = [org, where[_].text, a[4].text, a[0].text, now_url]
                sub_df_lst.append(new_row)
                break
            break
    return sub_df_lst


# In[34]:


def primo_crawler(driver, org, url_front, ISBN ,url_behind, tcn):
    table = []
    
    url = url_front + ISBN + url_behind
    primo_lst = []

    try:
        # é€²å…¥ã€Šé¤¨è—ç³»çµ±ã€‹é é¢
        driver.get(url)
        time.sleep(8)

        try: #é–‹å§‹çˆ¬èŸ²
            editions = driver.find_elements_by_class_name('item-title') 
            if len(editions) > 1: #å¦‚æœæœ€å¤–é¢æœ‰å…©å€‹ç‰ˆæœ¬(é»˜èªé»é€²å»ä¸æœƒå†åˆ†ç‰ˆæœ¬äº†å•¦)(ex.æ”¿å¤§ 9789861371955)ï¼Œç›´æ¥äº¤çµ¦ä¸‹é¢è™•ç†
                pass
            else: #å¦‚æœæœ€å¤–é¢åªæœ‰ä¸€å€‹ç‰ˆæœ¬ï¼Œé‚£æœ‰å¯èƒ½é»é€²å»é‚„æœ‰å†åˆ†ï¼Œå…ˆclické€²å»ï¼Œå†åˆ†ä¸€å€‹ç‰ˆæœ¬è·Ÿå¤šå€‹ç‰ˆæœ¬çš„ç‹€æ³
                time.sleep(5)
                editions[0].click()
                time.sleep(5)
                editions = driver.find_elements_by_class_name('item-title') #é€™æ™‚å€™æ˜¯ç¬¬äºŒå±¤çš„åˆ†ç‰ˆæœ¬äº†ï¼(ex.æ”¿å¤§ 9789869109321)
                
            try: #å…ˆæ‰¾å‰å‰ç¢ºå®šæ˜¯ä¸æ˜¯åœ¨æœ€è£¡å±¤äº†
                back_check = driver.find_element_by_class_name("md-icon-button.close-button.full-view-navigation.md-button.md-primoExplore-theme.md-ink-ripple")
            except:
                back_check = None
            if back_check == None: #å¤šå€‹ç‰ˆæœ¬æ‰è¦å†è·‘è¿´åœˆ(æ‰¾ä¸åˆ°å‰å‰ä»£è¡¨ä¸åœ¨æœ€è£¡é¢ï¼Œå¯çŸ¥ä¸æ˜¯ä¸€å€‹ç‰ˆæœ¬)
                for i in range(0, len(editions)): #æœ‰å¹¾å€‹ç‰ˆæœ¬å°±è·‘å¹¾æ¬¡ï¼Œä¸ç®¡å“ªä¸€å±¤ç‰ˆæœ¬éƒ½é©ç”¨
                    time.sleep(5)
                    into = editions[i].click()
                    time.sleep(10)
                    primo_lst += primo_finding(org, tcn, driver)
                    table = pd.concat(primo_lst, axis=0, ignore_index=True)
                    try: 
                        back2 = driver.find_element_by_class_name("md-icon-button.close-button.full-view-navigation.md-button.md-primoExplore-theme.md-ink-ripple").click()
                    except:
                        back2 = None

            else: #å¦‚æœåªæœ‰ä¸€å€‹ç‰ˆæœ¬(æœ‰å‰å‰çš„æ„æ€)ï¼Œé‚£å‰é¢å·²ç¶“clickéäº†ä¸èƒ½å†åš
                time.sleep(12)
                primo_lst += primo_finding(driver, org, tcn)
                table = pd.DataFrame(primo_lst)
                table.rename(columns={0: 'åœ–æ›¸é¤¨', 1: 'é¤¨è—åœ°', 2: 'ç´¢æ›¸è™Ÿ', 3: 'é¤¨è—ç‹€æ…‹', 4: 'é€£çµ'}, inplace=True)
        except:
            pass
    except:
        pass
    return table


# In[35]:


# # å¾…æ¸¬è©¦
# driver = webdriver.Chrome()
# table = primo_crawler(
#     driver=driver,
#     org='åœ‹ç«‹è‡ºç£å¤§å­¸',
#     url_front='https://ntu.primo.exlibrisgroup.com/discovery/search?query=any,contains,',
#     ISBN='9789573317241',
#     url_behind='&tab=Everything&search_scope=MyInst_and_CI&vid=886NTU_INST:886NTU_INST&offset=0',
#     tcn='layout-align-space-between-center.layout-row.flex-100'
# )
# table


# In[ ]:





# In[ ]:





# In[36]:


# primo_crawler(
#     driver=driver,
#     org='åœ‹ç«‹æ”¿æ²»å¤§å­¸',
#     url_front='https://nccu.primo.exlibrisgroup.com/discovery/search?query=any,contains,',
#     ISBN=ISBN,
#     url_behind='&tab=Everything&search_scope=MyInst_and_CI&vid=886NCCU_INST:886NCCU_INST',
#     tcn='layout-align-space-between-center.layout-row.flex-100'
# )
# primo_crawler(
#     driver=driver,
#     org="éŠ˜å‚³å¤§å­¸",
#     url_front="https://uco-mcu.primo.exlibrisgroup.com/discovery/search?query=any,contains,",
#     ISBN=ISBN,
#     url_behind="&tab=Everything&search_scope=MyInst_and_CI&vid=886UCO_MCU:886MCU_INST&lang=zh-tw&offset=0",
#     tcn="md-2-line.md-no-proxy._md"
# )
# primo_crawler(
#     driver=driver,
#     org="æ±å³å¤§å­¸",
#     url_front="https://uco-scu.primo.exlibrisgroup.com/discovery/search?query=any,contains,",
#     ISBN=ISBN,
#     url_behind"&tab=Everything&search_scope=MyInst_and_CI&vid=886UCO_SCU:886SCU_INST&lang=zh-tw&offset=0",
#     tcn="md-2-line.md-no-proxy._md"
# )


# In[ ]:





# In[ ]:





# ## <font color='red'>é€²è¡Œä¸­</font>å°ä¸­ç§‘æŠ€å¤§å­¸

# In[ ]:





# In[ ]:





# In[ ]:





# # é€™ç¶²é ä¹Ÿå¤ªçˆ›äº†å§â€¦â€¦

# ## <font color='red'>å¾…ç¶­ä¿®</font>åŸºéš†å¸‚å…¬å…±åœ–æ›¸é¤¨(driver, org, org_url, ISBN) å¾ˆå¥‡æ€ª
# - ã€æœ€å¾Œç·¨è¼¯ã€ï¼š
# - ã€å‡½å¼å®Œæˆåº¦ã€ï¼šä¸­

# ### å‡½å¼èªªæ˜
# - ã€é‹ä½œçš„åŸç†ã€ï¼šä½¿ç”¨ Selenium
# - ã€é©ç”¨çš„æ©Ÿæ§‹ã€ï¼š[åŸºéš†å¸‚å…¬å…±åœ–æ›¸é¤¨](https://webpac.klccab.gov.tw/webpac/search.cfm)
# - ã€èƒ½è™•ç†ç‹€æ³ã€ï¼šä¸€ç­†ã€ç„¡
# - ã€ä¸‹ä¸€æ­¥å„ªåŒ–ã€ï¼š
#     - ç¶²ç«™è¼‰å…¥éæ…¢ï¼Œä¸” wait æ–¹å¼ä¸é©ç”¨æ–¼æ­¤ï¼Œåªèƒ½ä½¿ç”¨å¤§é‡çš„ time.sleep()

# ### å‡½å¼æœ¬é«”

# In[37]:


# def åŸºéš†å¸‚å…¬å…±åœ–æ›¸é¤¨(driver, org, org_url, ISBN):
#     try:
#         driver.get(org_url)
#         wait_for_element_clickable(driver, 'é€²éšæª¢ç´¢').click()  # é»æ“Šï¼‚é€²éšæª¢ç´¢ï¼‚
#         time.sleep(2)  # JavaScript å‹•ç•«ï¼Œå¼·åˆ¶ç­‰å¾…
#         select_ISBN_strategy(driver, 'as_type_1', 'i')
#         search_ISBN(driver, ISBN, 'as_keyword_1')

#         time.sleep(8)  # åŸºéš†çš„ç³»çµ±å¤ªè©­ç•°äº†ï¼Œå¼·åˆ¶ç­‰å¾…
#         soup = BeautifulSoup(driver.page_source, "html.parser")
#         results = len(soup.find_all("div", "list_box"))
#         if results < 2:
#             WebDriverWait(driver, 10).until(
#                 EC.presence_of_element_located(
#                     (By.CSS_SELECTOR, "table.list.list_border")))
#             time.sleep(2)
#             table = pd.read_html(driver.page_source)[0]
#         else:
#             table = []
#             for li in soup.find_all("div", "list_box"):
#                 url_temp = "https://webpac.klccab.gov.tw/webpac/" + li.find(
#                     "a", "btn")["href"]
#                 driver.get(url_temp)
#                 wait.until(
#                     EC.presence_of_element_located(
#                         (By.CSS_SELECTOR, "table.list.list_border")))
#                 time.sleep(2)
#                 table.append(
#                     pd.read_html(driver.page_source, encoding="utf-8")[0])
#             table = pd.concat(table, axis=0, ignore_index=True)
#         table['åœ–æ›¸é¤¨'], table['é€£çµ'] = org, driver.current_url
#         table = organize_columns(table)
#         return table
#     except:
#         print(f'ã€Š{ISBN}ã€‹åœ¨ã€Œ{url}ã€ç„¡æ³•çˆ¬å–')


# In[38]:


# def åŸºéš†å¸‚å…¬å…±åœ–æ›¸é¤¨(driver, org, org_url, ISBN):
#     table = []

#     driver.get(org_url)
#     search_ISBN(driver, ISBN, 'ss_keyword')

#     time.sleep(8)  # åŸºéš†çš„ç³»çµ±å¤ªè©­ç•°äº†ï¼Œå¼·åˆ¶ç­‰å¾…

#     if wait_for_element_present(driver, '.list_border'):  # ä¸€ç­†
#         tgt = accurately_find_table_and_read_it(driver, '.list_border')
#         tgt['åœ–æ›¸é¤¨'], tgt['é€£çµ'] = org, driver.current_url
#         table.append(tgt)
#     elif wait_for_element_clickable(driver, 'è©³ç´°æ›¸ç›®'):  # å¤šç­†
#         tgt_urls = []
#         anchors = driver.find_elements_by_link_text('è©³ç´°æ›¸ç›®')
#         for anchor in anchors:
#             tgt_urls.append(anchor.get_attribute('href'))

#         for tgt_url in tgt_urls:
#             driver.get(tgt_url)

#             if not wait_for_element_present(driver, '.list_border'):
#                 continue
#             tgt = accurately_find_table_and_read_it(driver, '.list_border')
#             tgt['åœ–æ›¸é¤¨'], tgt['é€£çµ'] = org, driver.current_url
#             table.append(tgt)
#     table = organize_columns(table)
#     return table

